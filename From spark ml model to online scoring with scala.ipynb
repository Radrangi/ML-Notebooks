{
    "cells": [
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "<table style=\"border: none\" align=\"left\">\n   <tr style=\"border: none\">\n      <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Predict outdoor equipment purchase with IBM Watson Machine Learning</b></th>\n      <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"40\" width=\"40\"></th>\n   </tr>\n   <tr style=\"border: none\">\n       <th style=\"border: none\"><img src=\"https://github.com/pmservice/wml-sample-models/blob/master/spark/product-line-prediction/images/products_graphics.png?raw=true\" alt=\"Icon\" width=\"800\"> </th>\n   </tr>\n</table>"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "This notebook contains steps and code to get data from the IBM Data Science Experience Community, create a predictive model, and start scoring new data. This notebook introduces commands for getting data and for basic data cleaning and exploration, pipeline creation, model training, model persistance to Watson Machine Learning repository, model deployment, and scoring.\n\nSome familiarity with Scala is helpful. This notebook uses Scala 2.11 and Apache\u00ae Spark 2.0.\n\nYou will use a publicly available data set, **GoSales Transactions for Naive Bayes Model**, which details anonymous outdoor equipment purchases. Use the details of this data set to predict clients' interests in terms of product line, such as golf accessories, camping equipment, and others.\n\n## Learning goals\n\nThe learning goals of this notebook are:\n\n-  Load a CSV file into an Apache\u00ae Spark DataFrame.\n-  Explore data.\n-  Prepare data for training and evaluation.\n-  Create an Apache\u00ae Spark machine learning pipeline.\n-  Train and evaluate a model.\n-  Persist a pipeline and model in Watson Machine Learning repository.\n-  Deploy a model for online scoring using Wastson Machine Learning API.\n-  Score sample scoring data using the Watson Machine Learning API.\n\n\n## Contents\n\nThis notebook contains the following parts:\n\n1.\t[Setup](#setup)\n2.\t[Load and explore data](#load)\n3.\t[Create spark ml model](#model)\n4.\t[Persist model](#persistence)\n5.\t[Predict locally and visualize](#visualization)\n6.\t[Deploy and score in a Cloud](#scoring)\n7.\t[Summary and next steps](#summary)"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<a id=\"setup\"></a>\n## 1. Setup\n\nBefore you use the sample code in this notebook, you must perform the following setup tasks:\n\n-  Create a [Watson Machine Learning Service](https://console.ng.bluemix.net/catalog/services/ibm-watson-machine-learning/) instance (a free plan is offered). \n-  Upload **GoSales Transactions** data as a data asset in IBM Data Science Experience.\n-  Make sure that you are using a Spark 2.0 kernel.\n\n### Create the GoSales Transactions Data Asset  \n\nThe GOSales data is a freely available data set on the Data Science Experience home page.\n\n1.  Go to the [GoSales Transactions for Naive Bayes Model](https://apsportal.ibm.com/exchange-api/v1/entries/8044492073eb964f46597b4be06ff5ea/data?accessKey=9561295fa407698694b1e254d0099600) data card on the Data Science Experience **Community** page and open the card by double-clicking it.\n2.  Click the link icon.\n4.  Select the link, copy it by pressing Ctrl+C, and then, click **Close**.\n5.  In the following cell, replace the **link_to_data** variable value with the link."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<a id=\"load\"></a>\n## 2. Load and explore data\n\nIn this section you will load the data as an Apache\u00ae Spark DataFrame and perform a basic exploration.\n\nLoad the data to the Spark DataFrame by using *wget* to upload the data to gpfs and then *read* method. "
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "import scala.sys.process._\n\n\"wget https://apsportal.ibm.com/exchange-api/v1/entries/8044492073eb964f46597b4be06ff5ea/data?accessKey=9561295fa407698694b1e254d0099600\".!", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val filename = \"data?accessKey=9561295fa407698694b1e254d0099600\"", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val df_data = spark.\n    read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").\n    option(\"header\", \"true\").\n    option(\"inferSchema\", \"true\").\n    load(filename)", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Explore the loaded data by using the following Apache\u00ae Spark DataFrame methods:\n-  print schema\n-  print top ten records\n-  count all records"
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "df_data.printSchema()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "As you can see, the data contains five fields. PRODUCT_LINE field is the one we would like to predict (label)."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "df_data.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "println(\"Total number of records: \" + df_data.count())", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "As you can see, the data set contains 60252 records."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<a id=\"model\"></a>\n## 3. Create an Apache\u00ae Spark machine learning model\n\nIn this section you will learn how to prepare data, create an Apache\u00ae Spark machine learning pipeline, and train a model."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 3.1: Prepare data\n\nIn this subsection you will split your data into: train, test and predict datasets."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val splits = df_data.randomSplit(Array(0.8, 0.18, 0.02), seed = 24L)\nval training_data = splits(0).cache()\nval test_data = splits(1)\nval prediction_data = splits(2)\n\nprintln(\"Number of training records: \" + training_data.count())\nprintln(\"Number of testing records: \" + test_data.count())\nprintln(\"Number of prediction records: \" + prediction_data.count())", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "As you can see our data has been successfully split into three datasets: \n\n-  The train data set, which is the largest group, is used for training.\n-  The test data set will be used for model evaluation and is used to test the assumptions of the model.\n-  The predict data set will be used for prediction."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 3.2: Create pipeline and train a model"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "In this section you will create an Apache\u00ae Spark machine learning pipeline and then train the model.\n\nIn the first step you need to import the Apache\u00ae Spark machine learning packages that will be needed in the subsequent steps."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "// Spark ML libraries\n\nimport org.apache.spark.ml.classification.RandomForestClassifier\nimport org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer, IndexToString, VectorAssembler}\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.{Model, Pipeline, PipelineStage, PipelineModel}\nimport org.apache.spark.sql.SparkSession", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "In the following step, convert all the string fields to numeric ones by using the StringIndexer transformer."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val stringIndexer_label = new StringIndexer().setInputCol(\"PRODUCT_LINE\").setOutputCol(\"label\").fit(df_data)\nval stringIndexer_prof = new StringIndexer().setInputCol(\"PROFESSION\").setOutputCol(\"PROFESSION_IX\")\nval stringIndexer_gend = new StringIndexer().setInputCol(\"GENDER\").setOutputCol(\"GENDER_IX\")\nval stringIndexer_mar = new StringIndexer().setInputCol(\"MARITAL_STATUS\").setOutputCol(\"MARITAL_STATUS_IX\")", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "In the following step, create a feature vector by combining all features together."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val vectorAssembler_features = new VectorAssembler().setInputCols(Array(\"GENDER_IX\", \"AGE\", \"MARITAL_STATUS_IX\", \"PROFESSION_IX\")).setOutputCol(\"features\")", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Next, define estimators you want to use for classification. Random Forest is used in the following example."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val rf = new RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\").setNumTrees(10)", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Finally, indexed labels back to original labels."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val labelConverter = new IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(stringIndexer_label.labels)", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Let's build the pipeline now. A pipeline consists of transformers and an estimator."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val pipeline_rf = new Pipeline().setStages(Array(stringIndexer_label, stringIndexer_prof, stringIndexer_gend, stringIndexer_mar, vectorAssembler_features, rf, labelConverter))", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Now, you can train your Random Forest model by using the previously defined **pipeline** and **training data**."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "training_data.printSchema()", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val model_rf = pipeline_rf.fit(training_data)", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "You can check your **model accuracy** now. To evaluate the model, use **test data**."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val predictions = model_rf.transform(test_data)\nval evaluatorRF = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\nval accuracy = evaluatorRF.evaluate(predictions)\n\nprintln(\"Accuracy = \" + accuracy)\nprintln(\"Test Error = \" + (1.0 - accuracy))", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "You can tune your model now to achieve better accuracy. For simplicity of this example tuning section is omitted."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<a id=\"persistence\"></a>\n## 4. Persist model\n\nIn this section you will learn how to store your pipeline and model in Watson Machine Learning repository by using Scala client libraries.\n\nFirst, you must import client libraries.\n\n**Note**: Apache\u00ae Spark 2.0 or higher is required."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "// WML client library\n\nimport com.ibm.analytics.ngp.repository._\n\n// Helper libraries\n\nimport scalaj.http.{Http, HttpOptions}\nimport scala.util.{Success, Failure}\nimport java.util.Base64\nimport java.nio.charset.StandardCharsets\nimport play.api.libs.json._", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Authenticate to Watson Machine Learning service on Bluemix.\n\n**Action**: Put authentication information from your instance of Watson Machine Learning service here.</div>"
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val wml_service_path = \"https://ibm-watson-ml.mybluemix.net\"\nval wml_instance_id = \"***\"\nval wml_username = \"***\"\nval wml_password = \"***\"", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "**Tip**: `wml_service_path`, `wml_user` and `wml_password` can be found on **Service Credentials** tab of service instance created in Bluemix. If you cannot see **instance_id** field in **Serice Credentials** generate new credentials by pressing **New credential (+)** button. "
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val client = MLRepositoryClient(wml_service_path)\nclient.authorize(wml_username, wml_password)", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Create model artifact (abstraction layer)."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val model_artifact = MLRepositoryArtifact(model_rf, training_data, \"WML Product Line Prediction Model\")", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "**Tip**: The MLRepositoryArtifact method expects a trained model object, training data, and a model name. (It is this model name that is displayed by the Watson Machine Learning service)."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 4.1: Save pipeline and model\n\nIn this subsection you will learn how to save pipeline and model artifacts to your Watson Machine Learning instance."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val saved_model = client.models.save(model_artifact).get", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Get saved model metadata from Watson Machine Learning.\n\n**Tip**: Use *meta.availableProps* to get the list of available props."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "saved_model.meta.availableProps", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "println(\"modelType: \" + saved_model.meta.prop(\"modelType\"))\nprintln(\"trainingDataSchema: \" + saved_model.meta.prop(\"trainingDataSchema\"))\nprintln(\"creationTime: \" + saved_model.meta.prop(\"creationTime\"))\nprintln(\"modelVersionHref: \" + saved_model.meta.prop(\"modelVersionHref\"))\nprintln(\"label: \" + saved_model.meta.prop(\"label\"))", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "**Tip**: **modelVersionHref** is our model unique indentifier in the Watson Machine Learning repository."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 4.2: Load model and make predictions\n\nIn this subsection you will learn how to load back saved model from specified instance of Watson Machine Learning."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val model_version_href = saved_model.meta.prop(\"modelVersionHref\").get\nval loaded_model_artifact = client.models.version(model_version_href).get", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "You can print for example model name to make sure that model artifact has been loaded correctly."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "loaded_model_artifact.name.mkString", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "As you can see the name is correct. "
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "loaded_model_artifact match {\n        case SparkPipelineModelLoader(Success(model)) => {\n          val predictions = model.transform(prediction_data)\n        }\n        case SparkPipelineModelLoader(Failure(e)) => \"Loading failed.\"\n        case _ => println(s\"Unexpected artifact class: ${loaded_model_artifact.getClass}\")\n    }\npredictions.select(\"GENDER\", \"AGE\", \"MARITAL_STATUS\", \"PROFESSION\", \"predictedLabel\").show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "By tabulating a count, you can see which product line is the most popular."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "predictions.select(\"predictedLabel\").groupBy(\"predictedLabel\").count().show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "You have already learned how save and load the model from Watson Machine Learning repository."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<a id=\"scoring\"></a>\n## 5. Deploy and score in a Cloud\n\nIn this section you will learn how to create online scoring and to score a new data record by using the Watson Machine Learning REST API. \nFor more information about REST APIs, see the [Swagger Documentation](http://watson-ml-api.mybluemix.net/).\n\nTo work with the Watson Machine Leraning REST API you must generate an access token. To do that you can use the following sample code:"
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "// Get WML service instance token\n\nval wml_auth_header = \"Basic \" + Base64.getEncoder.encodeToString((wml_username + \":\" + wml_password).getBytes(StandardCharsets.UTF_8))\nval wml_url = wml_service_path + \"/v3/identity/token\"\nval wml_response = Http(wml_url).header(\"Authorization\", wml_auth_header).asString\nval wml_token_json:JsValue = Json.parse(wml_response.body)\n\nval wml_token = (wml_token_json \\ \"token\").asOpt[String] match {\n    case Some(x) => x\n    case None => \"\"\n}", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "println(wml_token)", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 5.1: Create online scoring endpoint\n\nNow you can create an online scoring endpoint. Execute the following sample code that uses the publishedModelId value to create the scoring endpoint to the Bluemix repository."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### Get published_models url from instance details"
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val endpoint_instance = service_path + \"/v3/wml_instances/\" + instance_id\nval wml_response_instance = Http(endpoint_instance).header(\"Content-Type\", \"application/json\").header(\"Authorization\", \"Bearer \" + wml_token).option(HttpOptions.connTimeout(10000)).option(HttpOptions.readTimeout(50000)).asString", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "wml_response_instance", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val published_models_json: JsValue = Json.parse(wml_response_instance.body)\nval published_models_url = (((published_models_json \\ \"entity\") \\\\ \"published_models\")(0) \\ \"url\").as[JsString].value\npublished_models_url", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### Get list of published models"
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val wml_models = Http(published_models_url).header(\"Content-Type\", \"application/json\").header(\"Authorization\", \"Bearer \" + wml_token).option(HttpOptions.connTimeout(10000)).option(HttpOptions.readTimeout(50000)).asString\nwml_models", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "var deployment_endpoint: String = _\nwml_models.body.split(\"\\\"\").map{ s => {if ((s contains \"deployments\") & (s contains saved_model.uid.mkString)) {deployment_endpoint = s}}}\ndeployment_endpoint", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### Create online deployment for published model"
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val payload_name = \"Online scoring\"\nval payload_data_online = Json.stringify(Json.toJson(Map(\"type\" -> \"online\", \"name\" -> payload_name)))", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val response_online = Http(deployment_endpoint).postData(payload_data_online).header(\"Content-Type\", \"application/json\").header(\"Authorization\", \"Bearer \" + wml_token).option(HttpOptions.connTimeout(50000)).option(HttpOptions.readTimeout(50000)).asString", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val scoring_url_json: JsValue = Json.parse(response_online.body)\nval scoring_url = (scoring_url_json \\ \"entity\" \\ \"scoring_url\").asOpt[String] match {\n    case Some(x) => x\n    case None => \"\"\n}", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "print(scoring_url)", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val payload_scoring = Json.stringify(Json.toJson(Map(\"fields\" -> Json.toJson(List(Json.toJson(\"GENDER\"), Json.toJson(\"AGE\"), Json.toJson(\"MARITAL_STATUS\"), Json.toJson(\"PROFESSION\"))),\n                                                    \"values\" -> Json.toJson(List(List(Json.toJson(\"M\"), Json.toJson(55), Json.toJson(\"Single\"), Json.toJson(\"Executive\")))))))", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "payload_scoring", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Now, you can send (POST) new scoring records (new data) for which you would like to get predictions. To do that, execute the following sample code: "
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "val response_scoring = Http(scoring_url).postData(payload_scoring).header(\"Content-Type\", \"application/json\").header(\"Authorization\", \"Bearer \" + wml_token).option(HttpOptions.method(\"POST\")).option(HttpOptions.connTimeout(10000)).option(HttpOptions.readTimeout(50000)).asString", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "scrolled": false, 
                "collapsed": true
            }, 
            "source": "print(response_scoring)", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "As we can see we predict that a 55-year-old single male executive is interested in Mountaineering Equipment (prediction: 2.0)."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<a id=\"summary\"></a>\n## 6. Summary and next steps \n\nYou successfully completed this notebook! You learned how to use Apache Spark machine learning as well as Watson Machine Learning for model creation and deployment. Check out our _[Online Documentation](https://console.ng.bluemix.net/docs/services/PredictiveModeling/pm_service_api_spark.html)_ for more samples, tutorials, documentation, how-tos, and blog posts. \n \n### Authors\n\n**Umit Mert Cakmak** is Data Scientist in IBM with a track record of developing enterprise-level applications that substantially increases clients' ability to turn data into actionable insights.\n\nCopyright \u00a9 2017 IBM. This notebook and its source code are released under the terms of the MIT License."
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Scala 2.11 with Spark 2.0", 
            "name": "scala-spark20", 
            "language": "scala"
        }, 
        "language_info": {
            "version": "2.11.8", 
            "mimetype": "text/x-scala", 
            "codemirror_mode": "text/x-scala", 
            "file_extension": ".scala", 
            "name": "scala", 
            "pygments_lexer": "scala"
        }
    }, 
    "nbformat_minor": 1
}